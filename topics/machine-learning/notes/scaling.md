# 📏 특성 스케일링 (Feature Scaling)

특성 스케일링은 머신러닝에서 **입력 데이터의 범위를 조정**하는 과정으로,  
**학습 속도 향상**, **성능 개선**, **수렴 안정성 확보**를 위해 필수적입니다.

---

## 📌 왜 스케일링이 필요한가?

- 모델에 따라 입력 값의 **크기 차이가 학습에 큰 영향을 미침**  
- 거리 기반 알고리즘 (KNN, SVM 등), 경사하강법 기반 모델 (Linear Regression, Neural Network 등)에서 필수

```
예: 나이(20~60) vs 연봉(2000~10000) — 스케일이 다르면 연봉이 모델에 더 큰 영향
```

---

## 📊 주요 스케일링 방법

| 방법 | 수식/기술 | 특징 | 적용 예시 |
|------|------------|------|------------|
| **표준화 (Standardization)** | \`z = (x - μ) / σ\` | 평균 0, 표준편차 1 | 정규분포 가정, SVM, 로지스틱 회귀 등 |
| **정규화 (Min-Max Scaling)** | \`x' = (x - min) / (max - min)\` | 0~1 범위로 축소 | 거리 기반 모델, 이미지 픽셀값 처리 등 |
| **Robust Scaling** | \`x' = (x - Q1) / IQR\` | 이상치 영향 적음 | 이상치가 많은 데이터 |
| **MaxAbs Scaling** | \`x' = x / max(|x|)\` | [-1, 1] 범위 | 희소 데이터(sparse data), L1/L2 정규화 |

---

## 🔍 시각적 비교

```
Before Scaling:          After Min-Max Scaling:
[  25,  300,  1000 ]  →   [ 0.0, 0.3, 1.0 ]
```

---

## 🧠 머신러닝에서의 활용

| 항목 | 설명 |
|------|------|
| 정규화 대상 | 거리 기반 알고리즘 (KNN, SVM, KMeans), PCA 등 |
| 표준화 대상 | 선형 회귀, 로지스틱 회귀, 신경망 등 |
| 로그변환 | 정규화 전 왜도 줄이기 (특히 양의 값 분포에) |
| 파이프라인 | Scikit-learn \`StandardScaler\`, \`MinMaxScaler\` 등 제공 |

---

## 💡 실무 팁

- **훈련 데이터로 스케일링 기준을 계산하고**, 테스트 데이터에도 **같은 기준** 적용  
- `Pipeline`을 사용하면 스케일링과 모델 학습을 일괄 처리 가능  
- 이상치 존재 시 `RobustScaler` 추천  
- 정규분포 기반 모델에는 **StandardScaler**, 거리 기반 모델에는 **MinMaxScaler**가 일반적
