# 🧠 지도 학습 / 비지도 학습 / 강화 학습

머신러닝은 **학습 방법에 따라 크게 세 가지**로 분류됩니다:

- **지도 학습 (Supervised Learning)**
- **비지도 학습 (Unsupervised Learning)**
- **강화 학습 (Reinforcement Learning)**

각 학습 방식은 학습 목적과 사용되는 데이터 형태가 다릅니다.

---

## 1️⃣ 지도 학습 (Supervised Learning)

### 🔍 정의
정답(label)이 있는 데이터를 기반으로 **입력 → 출력 간 관계를 학습**하는 방식입니다.

### 📦 입력 데이터 형태  
```  
X (입력 데이터) → y (정답 라벨)  
```

### 💡 대표 문제 유형
- **분류 (Classification)**: 이메일이 스팸인지 아닌지
- **회귀 (Regression)**: 집값 예측, 온도 예측

### 🧠 주요 알고리즘
- 선형 회귀 (Linear Regression)
- 로지스틱 회귀 (Logistic Regression)
- 의사결정나무 (Decision Tree)
- KNN, SVM, Random Forest, XGBoost

### ✅ 예시
- 손글씨 숫자 이미지 → 숫자 라벨(0~9)
- 고객 정보 → 구매 여부 예측 (0 또는 1)

---

## 2️⃣ 비지도 학습 (Unsupervised Learning)

### 🔍 정의
정답(label)이 없는 데이터를 통해 **데이터 내 숨은 구조나 패턴을 찾는 방식**입니다.

### 📦 입력 데이터 형태  
```  
X (입력 데이터만 존재)  
```

### 💡 대표 문제 유형
- **클러스터링 (Clustering)**: 비슷한 고객끼리 묶기
- **차원 축소 (Dimensionality Reduction)**: 시각화, 노이즈 제거

### 🧠 주요 알고리즘
- K-Means, DBSCAN
- 계층적 군집 (Hierarchical Clustering)
- 주성분 분석 (PCA)
- t-SNE, Autoencoder

### ✅ 예시
- 웹사이트 방문 데이터를 기반으로 사용자 유형 군집화
- 이미지 압축, 노이즈 제거

---

## 3️⃣ 강화 학습 (Reinforcement Learning)

### 🔍 정의
**환경과 상호작용하면서 보상을 통해 최적의 행동을 학습**하는 방식입니다.  
즉, 시행착오를 통해 문제를 해결하는 에이전트를 학습시킵니다.

### 📦 구성 요소
- **Agent**: 학습하는 주체 (예: 로봇, 프로그램)
- **Environment**: 에이전트가 행동하는 공간
- **State (상태)**, **Action (행동)**, **Reward (보상)**

### 💡 핵심 특징
- 지도/비지도와 달리, **정답 데이터가 없음**
- 보상을 최대화하기 위한 전략(Policy)을 학습

### 🧠 주요 알고리즘
- Q-Learning
- SARSA
- Deep Q Network (DQN)
- Policy Gradient, A3C, PPO

### ✅ 예시
- 알파고: 바둑에서 승리하도록 스스로 학습
- 자율 주행차: 주행 중 최적 경로 및 행동 선택
- 로봇 제어, 게임 AI

---

## 📊 학습 방식 비교표

| 항목 | 지도 학습 | 비지도 학습 | 강화 학습 |
|------|-----------|--------------|------------|
| 입력 데이터 | X, y 쌍 | X만 있음 | 상태, 보상, 행동 |
| 학습 목적 | 예측 모델 생성 | 데이터 패턴 탐색 | 최적 정책 학습 |
| 라벨 필요 여부 | 필요함 | 없음 | 없음 (보상만 있음) |
| 주요 예시 | 이미지 분류, 가격 예측 | 군집화, 차원 축소 | 게임, 로봇 제어 |
| 데이터 수집 난이도 | 중간 | 쉬움 | 어렵고 비용 큼 |
