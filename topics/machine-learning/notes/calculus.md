# 📉 미분 & 경사하강법 (Gradient Descent)

머신러닝에서 모델을 학습한다는 것은 **오차를 줄이는 방향으로 파라미터를 조정하는 과정**입니다.  
이때 핵심이 되는 수학 개념이 **미분과 경사하강법**입니다.

---

## ✏️ 미분 (Differentiation)

미분은 함수의 기울기를 구하는 연산입니다.  
즉, **변수가 조금 변할 때 함수 값이 얼마나 변하는지를 나타냅니다.**

```plaintext
기울기 = 순간 변화율 = 도함수 f'(x)
```

### ✅ 예시

```plaintext
f(x) = x²  
→ f'(x) = 2x
```

| x | f(x) | f'(x) |
|--|------|--------|
| -2 | 4 | -4 |
| 0 | 0 |  0 |
| 2 | 4 |  4 |

---

## 📉 경사하강법 (Gradient Descent)

경사하강법은 **비용 함수(cost function)의 기울기를 따라 파라미터를 조금씩 이동시키며 최소값을 찾는 최적화 알고리즘**입니다.

---

## ⚙️ 알고리즘 핵심 식

```plaintext
θ ← θ - α * ∇J(θ)
```

- θ: 파라미터 (예: 가중치 w)
- α: 학습률 (learning rate)
- ∇J(θ): 비용 함수 J에 대한 θ의 기울기 (gradient)

---

## 📉 시각적 개념

곡선 위에서 아래쪽 방향(기울기 반대 방향)으로 조금씩 내려가면서  
**최소값(minimum)** 을 찾아가는 과정입니다.

---

## 🧪 예제

```plaintext
f(w) = (w - 3)²  
f'(w) = 2(w - 3)

시작 w = 0, α = 0.1
→ 반복: w ← w - 0.1 * 2(w - 3)
```

결과적으로 w는 점점 3에 가까워지며 최소값에 수렴합니다.

---

## 🛠️ 종류

| 방식 | 설명 |
|------|------|
| 배치 경사하강법 (Batch GD) | 전체 데이터를 사용해 1번에 1스텝 |
| 확률적 경사하강법 (SGD) | 데이터를 1개씩 사용, 빠르지만 불안정 |
| 미니배치 경사하강법 | 일부 데이터(batch) 사용, 가장 일반적 |

---

## ⚠️ 주의할 점

- **학습률이 너무 크면** 발산할 수 있음  
- **너무 작으면** 수렴 속도가 매우 느림  
- 로컬 최소값, 정체점(기울기 0) 등도 고려 필요
